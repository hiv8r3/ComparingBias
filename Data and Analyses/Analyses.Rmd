---
title: Observer analyses
author: Hannah, 5/16/2017
output:
  html_document:
    highlight: pygments
    theme: cerulean
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(width=140)
require(dplyr)
require(tidyr)
require(ggplot2)
require(lme4)
require(lmerTest)
require(knitr)
require(reshape2)
```

Data for Study 1 was collected in Fall 2015. Data for Study 2 was collected in Fall 2016.

**In Study 1:**  
- 101 subjects participated  
- 8 subjects' AP data was discarded (see "Study1_badsubsAP.txt")  
- 9 subjects' WIT data was discarded (see "Study1_badsubsWIT.txt")    
*This left 93 subjects with AP data and 92 subjects with WIT data.*

Additionally, IMS/EMS data is missing for subs 3, 27, 53, 82, 88, 89, 90, and 101. 

**In Study 2:**  
- 206 subjects participated  
- 8 subjects' AP data was discarded (see "Study2_badsubsAP.txt")  
- 7 subjects' WIT data was discarded (see "Study2_badsubsWIT.txt")  
*This left 198 subjects with AP data and 199 subjects with WIT data.*

Additionally, IMS/EMS data is missing for 111, 189, and 201.

There were 48 trials for each condition in each task. In Study 2, each task was split into two sections so that participants could answer anxiety questions in the middle and end of each task.

### 1. Accuracy in each task:  
#### 2 (Prime: Black/White) x 2 (Target: gun/tool or positive/negative) rANOVA  
Only showing interaction indicating racial bias (Prime x Target). For calculation of effect sizes, see "7 Effect sizes.R"  
#### Study 1:  
**WIT**  
```{r data, echo = FALSE}
# For analyses with accuracy as DV, need trials to be grouped into conditions (ie number of errors in each condition)
s1.acc = read.delim("Study1_errCountLong.txt")

# separate by task
s1.acc.AP = s1.acc[s1.acc$Task == "AP",]
s1.acc.WIT = s1.acc[s1.acc$Task == "WIT",]

s1.acc.AP$TargetType = factor(s1.acc.AP$TargetType)
s1.acc.WIT$TargetType = factor(s1.acc.WIT$TargetType)
s1.acc.AP$Subject = factor(s1.acc.AP$Subject)
s1.acc.WIT$Subject = factor(s1.acc.WIT$Subject)

# Race x Valence on accuracy (WIT)
sum = aov(numErr ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s1.acc.WIT) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`

```

**AP**
``` {r AP, echo = FALSE}
# Race x Valence on accuracy (WIT)
sum = aov(numErr ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s1.acc.AP) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`
```


#### Study 2: 
Only showing interaction indicating racial bias (Prime x Target). For calculation of effect sizes, see "7 Effect sizes.R"  
**WIT**  
```{r data2, echo = FALSE}
# For analyses with accuracy as DV, need trials to be grouped into conditions (ie number of errors in each condition)
s2.acc = read.delim("Study2_errCountLong.txt")

# separate by task
s2.acc.AP = s2.acc[s2.acc$Task == "AP",]
s2.acc.WIT = s2.acc[s2.acc$Task == "WIT",]

s2.acc.AP$TargetType = factor(s2.acc.AP$TargetType)
s2.acc.WIT$TargetType = factor(s2.acc.WIT$TargetType)
s2.acc.AP$Subject = factor(s2.acc.AP$Subject)
s2.acc.WIT$Subject = factor(s2.acc.WIT$Subject)

# Race x Valence on accuracy (WIT)
sum = aov(numErr ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s2.acc.WIT) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`
```

**AP**
``` {r AP2, echo = FALSE}
# Race x Valence on accuracy (WIT)
sum = aov(numErr ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s2.acc.AP) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`
```

### 2. Comparing accuracy across tasks
##### - Correlation between performance bias scores
##### - Look at 3 way Prime x Target x Task interaction

#### Study 1
Excludes subjects that don't have data in both tasks (only includes sample of 90).  

#### A. Correlation between performance bias scores on each task
``` {r perfBias.s1, echo=F, warning=F}
s1.perfBias = read.delim("Study1_perfBias.txt")

# Look at correlation between tasks
ggplot(s1.perfBias, aes(APStand, WITStand)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "Stand. Performance bias on AP", y = "Stand. Performance bias on WIT") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(APStand ~ WITStand, data = s1.perfBias) %>%
  summary()
```

#### B. Examine 3 way Prime x Target x Task interaction  
``` {r 3way.s1, echo=F, wanring=F}
# Look just at subjects that have data for both tasks, otherwise throws error ("Error() model is singular")
# Probably because some subjects don't have data across both levels of task
s1.bsWIT = read.delim("Study1_badsubsWIT.txt")
s1.bsAP = read.delim("Study1_badsubsAP.txt")
s1.acc.nobs = s1.acc[!(s1.acc$Subject %in% s1.bsWIT$Subject) & !(s1.acc$Subject %in% s1.bsAP$Subject),]
s1.acc.nobs$Subject = factor(s1.acc.nobs$Subject)

# Total number of errors (looking at three-way interaction)
facet_labels <- c(AP = "APT", WIT = "WIT")

# Figure 1 ----------------------------------------------------------------
ggplot(s1.acc.nobs, aes(PrimeType, (48-numErr)/48, fill = ConType)) +
  stat_summary(fun.y = mean, geom = "bar", position = "dodge") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", position = position_dodge(width=.9), width = .2) +
  facet_wrap(~Task, labeller=labeller(Task = facet_labels)) + 
  #  ggtitle("Total number of errors") +
  labs(y = "Accuracy rate", x = "Race of Prime") +
  scale_fill_manual(values=c("black","grey70"), guide = guide_legend(title = NULL)) +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        strip.text.x = element_text(face = "bold", size = 14),
        strip.background = element_rect(fill = "grey98"),
        axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))

ggsave("./Figures/Accuracy_Study1.tiff")


# See if pattern of racial bias differs across two tasks- TOTAL ERRORS
sum2 = aov(numErr ~ (PrimeType*ConType*Task)+Error(Subject/(PrimeType*ConType*Task)), data = s1.acc.nobs) %>%
  summary()

sum2$`Error: Subject:PrimeType:ConType:Task`

```

#### Study 2
Excludes subjects that don't have data in both tasks (only includes sample of 195).  

#### A. Correlation between performance bias scores on each task
``` {r perfBias.s2, echo=F, warning=F}
s2.perfBias = read.delim("Study2_perfBias.txt")

# Look at correlation between tasks
ggplot(s2.perfBias, aes(APStand, WITStand)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "Stand. Performance bias on AP", y = "Stand. Performance bias on WIT") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(APStand ~ WITStand, data = s2.perfBias) %>%
  summary()
```

#### B. Examine 3 way Prime x Target x Task interaction  
``` {r 3way.s2, echo=F, wanring=F}
# Look just at subjects that have data for both tasks, otherwise throws error ("Error() model is singular")
# Probably because some subjects don't have data across both levels of task
s2.bsWIT = read.delim("Study2_badsubsWIT.txt")
s2.bsAP = read.delim("Study2_badsubsAP.txt")
s2.acc.nobs = s2.acc[!(s2.acc$Subject %in% s2.bsWIT$Subject) & !(s2.acc$Subject %in% s2.bsAP$Subject),]
s2.acc.nobs$Subject = factor(s2.acc.nobs$Subject)

# Total number of errors (looking at three-way interaction)
facet_labels <- c(AP = "APT", WIT = "WIT")

# Figure 1 ----------------------------------------------------------------
ggplot(s2.acc.nobs, aes(PrimeType, (48-numErr)/48, fill = ConType)) +
  stat_summary(fun.y = mean, geom = "bar", position = "dodge") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", position = position_dodge(width=.9), width = .2) +
  facet_wrap(~Task, labeller=labeller(Task = facet_labels)) + 
  #  ggtitle("Total number of errors") +
  labs(y = "Accuracy rate", x = "Race of Prime") +
  scale_fill_manual(values=c("black","grey70"), guide = guide_legend(title = NULL)) +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        strip.text.x = element_text(face = "bold", size = 14),
        strip.background = element_rect(fill = "grey98"),
        axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))

ggsave("./Figures/Accuracy_Study2.tiff")


# See if pattern of racial bias differs across two tasks- TOTAL ERRORS
sum2 = aov(numErr ~ (PrimeType*ConType*Task)+Error(Subject/(PrimeType*ConType*Task)), data = s2.acc.nobs) %>%
  summary()

sum2$`Error: Subject:PrimeType:ConType:Task`

```

### 3. Look at correlations between PDP estimates

####Study 1:  
``` {r correl, echo=F}
# compare PDP-C estimates across tasks
s1.widePDP = read.delim("Study1_PDP_wide.txt")

# C estimates
ggplot(s1.widePDP, aes(WIT_MeanC.stand, AP_MeanC.stand)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-C", y = "AP PDP-C") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT_MeanC.stand ~ AP_MeanC.stand, data = s1.widePDP) %>% summary()

# A estimates
ggplot(s1.widePDP, aes(WIT_AResid.stand, AP_AResid.stand)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-A", y = "AP PDP-A") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT_AResid.stand ~ AP_AResid.stand, data = s1.widePDP) %>% summary()

# Compare C and A resid 
tempC = select(s1.widePDP, Subject, WIT_MeanC.stand, AP_MeanC.stand) %>% 
  rename(WITestimate = WIT_MeanC.stand, APTestimate = AP_MeanC.stand)
tempC$Type = "PDP-C"

tempA = select(s1.widePDP, Subject, WIT_AResid.stand, AP_AResid.stand) %>% 
  rename(WITestimate = WIT_AResid.stand, APTestimate = AP_AResid.stand)
tempA$Type = "PDP-A"

compareAC = rbind(tempC, tempA)
compareAC$Type = factor(compareAC$Type)

ggplot(compareAC, aes(WITestimate, APTestimate, pch = Type)) +
  geom_point(aes(shape = Type), size = 2.5, alpha = .7) +
  scale_shape_manual(values=c(1,17)) +
  scale_linetype_manual(values=c("solid", "dashed")) +
  theme_bw() +
  geom_smooth(method = "lm", aes(linetype=Type), color = "black") +
  labs(x = "PDP estimates for WIT", y = "PDP estimates for APT") +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        legend.title = element_blank(),
        legend.key.size = unit(1.2, "cm"),
        axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        axis.text.x = element_text(size=16),
        axis.text.y = element_text(size=16))+
  coord_cartesian(ylim=c(-2.5,3), xlim=c(-2.5,3))

ggsave("./Figures/CompareAC_Study1.tiff")

lm(WITestimate ~ APTestimate*Type, data = compareAC) %>% summary()
lm(WITestimate ~ APTestimate+Type, data = compareAC) %>% summary()
```

####Study 2:  
``` {r correl2, echo=F}
# compare PDP-C estimates across tasks
s2.widePDP = read.delim("Study2_PDP_wide.txt")

# C estimates
ggplot(s2.widePDP, aes(WIT_MeanC.stand, AP_MeanC.stand)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-C", y = "AP PDP-C") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT_MeanC.stand ~ AP_MeanC.stand, data = s2.widePDP) %>% summary()

# A estimates
ggplot(s2.widePDP, aes(WIT_AResid.stand, AP_AResid.stand)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-A", y = "AP PDP-A") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT_AResid.stand ~ AP_AResid.stand, data = s2.widePDP) %>% summary()

# Compare C and A resid 
tempC = select(s2.widePDP, Subject, WIT_MeanC.stand, AP_MeanC.stand) %>% 
  rename(WITestimate = WIT_MeanC.stand, APTestimate = AP_MeanC.stand)
tempC$Type = "PDP-C"

tempA = select(s2.widePDP, Subject, WIT_AResid.stand, AP_AResid.stand) %>% 
  rename(WITestimate = WIT_AResid.stand, APTestimate = AP_AResid.stand)
tempA$Type = "PDP-A"

compareAC = rbind(tempC, tempA)
compareAC$Type = factor(compareAC$Type)

ggplot(compareAC, aes(WITestimate, APTestimate, pch = Type)) +
  geom_point(aes(shape = Type), size = 2.5, alpha = .7) +
  scale_shape_manual(values=c(1,17)) +
  scale_linetype_manual(values=c("solid", "dashed")) +
  theme_bw() +
  geom_smooth(method = "lm", aes(linetype=Type), color = "black") +
  labs(x = "PDP estimates for WIT", y = "PDP estimates for APT") +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        legend.title = element_blank(),
        legend.key.size = unit(1.2, "cm"),
        axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        axis.text.x = element_text(size=16),
        axis.text.y = element_text(size=16)) +
  coord_cartesian(ylim=c(-2.5,3), xlim=c(-2.5,3))

ggsave("./Figures/CompareAC_Study2.tiff")

lm(WITestimate ~ APTestimate*Type, data = compareAC) %>% summary()
lm(WITestimate ~ APTestimate+Type, data = compareAC) %>% summary()
```

### 4. Test effect of Observer on four outcomes(response accuracy bias, PDP-C, PDP-A, and anxiety)  
#####Study 1:  
``` {r obsIMS1, echo=F, warning=F}

WIT = select(s1.widePDP, Subject, Observer, WIT_MeanC, WIT_AResid) %>% 
  rename(MeanC = WIT_MeanC, AResid = WIT_AResid)
WIT$Task = "WIT"

APT = select(s1.widePDP, Subject, Observer, APT_MeanC, APT_AResid) %>% 
  rename(MeanC = APT_MeanC, AResid = APT_AResid)
APT$Task = "APT"

# put them together
s1.wide = rbind(WIT, APT)

# add response accuracy data
s1.wide$perfBias = NA
for (k in unique(s1.perfBias$Subject)) {
  s1.wide$perfBias[s1.wide$Subject == k & s1.wide$Task == "WIT"] = s1.perfBias$WITperfBias[s1.perfBias$Subject == k]
  s1.wide$perfBias[s1.wide$Subject == k & s1.wide$Task == "APT"] = s1.perfBias$APperfBias[s1.perfBias$Subject == k]  
}

# add anxiety
questDat = read.delim("Study1_questDat.txt")
questDat$blockName = as.character(questDat$blockName)
questDat$blockName[questDat$blockName == "PostWITquestions"] = "WIT"
questDat$blockName[questDat$blockName == "PostAPquestions"] = "AP"
questDat$blockName = as.factor(questDat$blockName)

names(questDat)[3] = "Resp"

questDat$SubTrial[questDat$SubTrial == 1] = "Frust"
questDat$SubTrial[questDat$SubTrial == 2] = "Anx"
questDat$SubTrial[questDat$SubTrial == 3] = "Unpleas"
questDat$SubTrial[questDat$SubTrial == 4] = "Attend"
questDat$SubTrial[questDat$SubTrial == 5] = "Effort"

questWide = spread(questDat, SubTrial, Resp)

questWide$Anx_comp = (questWide$Frust + questWide$Anx + questWide$Unpleas)/3

s1.wide$Anx_comp = NA
for (k in unique(questWide$Subject)) {
  s1.wide$Anx_comp[s1.wide$Subject == k & s1.wide$Task == "WIT"] = questWide$Anx_comp[questWide$Subject == k  & questWide$blockName == "WIT"]
  s1.wide$Anx_comp[s1.wide$Subject == k & s1.wide$Task == "APT"] = questWide$Anx_comp[questWide$Subject == k & questWide$blockName == "AP"]  
}

# Performance bias
m1 = aov(perfBias ~ Observer*Task + Error(Subject/(Task)), data = s1.wide) %>% 
  summary()
# PDP-A
m2 = aov(AResid ~ Observer*Task + Error(Subject/(Task)), data = s1.wide) %>% 
  summary()
# PDP-C
m3 = aov(MeanC ~ Observer*Task + Error(Subject/(Task)), data = s1.wide) %>% 
  summary()
# Anxiety composite
m4 = aov(Anx_comp ~ Observer*Task + Error(Subject/(Task)), data = s1.wide) %>% 
  summary()

```

**Perf bias:**
``` {r}
m1
```

**PDP-A estimates:**

``` {r}
m2
```

**PDP-C estimates:**

``` {r}
m3
```

**Anxiety composite:**

``` {r}
m4
```


#####Study 2:
``` {r}
# read in PDP estimates in wide format

WIT = select(s2.widePDP, Subject, Observer, WIT_MeanC, WIT_AResid) %>% 
  rename(MeanC = WIT_MeanC, AResid = WIT_AResid)
WIT$Task = "WIT"

APT = select(s2.widePDP, Subject, Observer, APT_MeanC, APT_AResid) %>% 
  rename(MeanC = APT_MeanC, AResid = APT_AResid)
APT$Task = "APT"

# put them together
s2.wide = rbind(WIT, APT)

# add response accuracy data
s2.wide$perfBias = NA
for (k in unique(s2.perfBias$Subject)) {
  s2.wide$perfBias[s2.wide$Subject == k & s2.wide$Task == "WIT"] = s2.perfBias$WITperfBias[s2.perfBias$Subject == k]
  s2.wide$perfBias[s2.wide$Subject == k & s2.wide$Task == "APT"] = s2.perfBias$APperfBias[s2.perfBias$Subject == k]  
}

# add anxiety later (see section 5)

# Performance bias
m5 = aov(perfBias ~ Observer*Task + Error(Subject/(Task)), data = s2.wide) %>% 
  summary()
# PDP-A
m6 = aov(AResid ~ Observer*Task + Error(Subject/(Task)), data = s2.wide) %>% 
  summary()
# PDP-C
m7 = aov(MeanC ~ Observer*Task + Error(Subject/(Task)), data = s2.wide) %>% 
  summary()

```

**Perf bias:**
``` {r}
m5
```

**PDP-A estimates:**

``` {r}
m6
```

**PDP-C estimates:**

``` {r}
m7
```

### 5. Make composite for anxiety in Study 2

``` {r composite, echo = F}
anxDat = read.delim("Study2_questDat.txt")
qplot(x=Var1, y=Var2, data=melt(cor(filter(anxDat, Subject != 87)[c(3:6, 8:18)])), fill=value, geom="tile") # sub 87 doesn't have data for the last four questions

hist(anxDat$Anx_composite, main = "Histogram for anxiety composite")
```

Composite is composed of 8 items (standardized before averaged together), alpha = .91

Items included in composite:
Rate your agreement with each statement from 1 (Strongly agree) to 6 (Strongly disagree):
 •	I feel a little self-conscious in this task.
 •	I am worried about some of the responses I have given during this task.
 •	I am feeling a bit uncomfortable with this task.
 •	I feel bothered that this task may reveal bias I wasn’t aware of.
While doing the task, to what extent did you experience these emotions? (scale is 1-not at all to 7-an extreme amount)
 -	Dread
 -	Anxiety
 -	Nervous
 -	Worry

Look at correlation between first and second blocks:  
``` {r composite2, echo = F, warning=F}

anxDat$Task = "APT"
anxDat$Task[grep("WIT", anxDat$blockName)] = "WIT"

block1 = anxDat[grep(1, anxDat$blockName),]
block2 = anxDat[grep(2, anxDat$blockName),]

block1 = select(block1, Subject, Task, Anx_composite) %>% 
  rename(Anx_composite1 = Anx_composite)
block2 = select(block2, Subject, Task, Anx_composite) %>% 
  rename(Anx_composite2 = Anx_composite)

sepBlock = cbind(block1, select(block2, Anx_composite2))

ggplot(sepBlock, aes(Anx_composite1, Anx_composite2, color = Task)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  geom_smooth(method="lm") +
  labs(x = "Block 1", y = "Block 2")

# have to take sub 87 out for correlation, has missing data
         
```

The correlation between the anxiety composite score on the first block and the second block is `r cor(sepBlock$Anx_composite1[sepBlock$Subject !=87], sepBlock$Anx_composite2[sepBlock$Subject !=87])`.  

### 6. Look at effect of Observer on anxiety composite. 

``` {r}
collapseBlock = select(anxDat, Subject, Task, Anx_composite) %>% 
  group_by(Subject, Task) %>% 
  summarise_each(funs(mean(., na.rm=T))) %>% 
  as.data.frame()

# add anxiety scores to s2.wide, which has PDP-C, PDP-A, Observer, and IMS
s2.wide$Anx_composite = NA
for (i in unique(s2.wide$Subject)) {
  for (t in c("WIT", "APT")) {
    s2.wide$Anx_composite[s2.wide$Subject == i & s2.wide$Task == t] = collapseBlock$Anx_composite[collapseBlock$Subject == i & collapseBlock$Task == t]
  }
}

m8 = aov(Anx_composite ~ Observer*Task + Error(Subject/(Task)), data = s2.wide) %>% 
  summary()
```

**Anxiety composite:**  
Anxiety composite scores were averaged across blocks, so each participant has one anxiety score per task.

``` {r}
m8
```

### 7. Look at relationship between anxiety and PDP-A/PDP-C

``` {r anx, echo=F}

ggplot(s2.wide, aes(scale(Anx_composite), MeanC, color = Task, shape = Task)) +
  geom_point() +
  geom_smooth(method="lm") +
  scale_color_manual(values=c("black", "forestgreen")) +
  scale_shape_manual(values=c(1,2)) +
  labs(x="Anxiety composite score", y = "PDP-C") +
#  ggtitle("Anxiety predicting PDP-C") +  
  theme_bw() +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        strip.text.x = element_text(face = "bold", size = 14),
        strip.background = element_rect(fill = "grey98"),
        axis.text.x = element_text(size = 16),
        axis.text.y = element_text(size = 16),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))
ggsave("./Figures/AnxPredictingPDPC.tiff")

lm1 = lmer(MeanC ~ scale(Anx_composite)*Task + (1|Subject), data = s2.wide) %>% summary()
kable(lm1$coefficients, digits = 3)

ggplot(s2.wide, aes(scale(Anx_composite), AResid, color = Task, shape = Task)) +
  geom_point() +
  geom_smooth(method="lm") +
  scale_color_manual(values=c("black", "forestgreen")) +
  scale_shape_manual(values=c(1,2)) +
  labs(x="Anxiety composite score", y = "PDP-A") +
#  ggtitle("Anxiety predicting PDP-A") +  
  theme_bw() +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        strip.text.x = element_text(face = "bold", size = 14),
        strip.background = element_rect(fill = "grey98"),
        axis.text.x = element_text(size = 16),
        axis.text.y = element_text(size = 16),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))
ggsave("./Figures/AnxPredictingPDPA.tiff")

lm2 = lmer(AResid ~ scale(Anx_composite)*Task + (1|Subject), data = s2.wide) %>% summary()
kable(lm2$coefficients, digits = 3)
```

### 8. Look at effect of race of experimenter
Experimenter is coded as "White" or "Nonwhite". Presence of observer, experimenter race, and task included as predictors of performance bias. 
``` {r}
dat = read.delim("Study2_experimentalTrials.txt")

selectDat = NULL
for (i in unique(dat$Subject)) {
  temp = dat[dat$Subject == i,]
  selectDat = rbind(selectDat, temp[1,])
}

selectDat = select(selectDat, Subject, Observer, IMS, Exp_ID, Exp_gender, Exp_race, DemoRace.RESP)
selectDat$Observer = as.character(selectDat$Observer)
selectDat$Exp_race = as.character(selectDat$Exp_race)
selectDat$Exp_gender = as.character(selectDat$Exp_gender)
selectDat$DemoRace.RESP = as.numeric(selectDat$DemoRace.RESP)


perfBias = read.delim("Study2_perfBias.txt")

melt = select(perfBias, Subject, WITperfBias, APperfBias) %>% 
  gather(Task, perfBias, WITperfBias, APperfBias)
melt$Task[melt$Task == "WITperfBias"] = "WIT"
melt$Task[melt$Task == "APperfBias"] = "APT"

for (i in unique(melt$Subject)) {
  melt$Observer[melt$Subject == i] = selectDat$Observer[selectDat$Subject == i]
  melt$IMS[melt$Subject == i] = selectDat$IMS[selectDat$Subject == i]
  melt$Exp_ID[melt$Subject == i] = selectDat$Exp_ID[selectDat$Subject == i]
  melt$Exp_gender[melt$Subject == i] = selectDat$Exp_gender[selectDat$Subject == i]
  melt$Exp_race[melt$Subject == i] = selectDat$Exp_race[selectDat$Subject == i]
  melt$Par_race[melt$Subject == i] = selectDat$DemoRace.RESP[selectDat$Subject == i]
}

melt$Exp_race2 = "Nonwhite"
melt$Exp_race2[melt$Exp_race == "White"] = "White"

melt$Par_race2 = "Nonwhite"
melt$Par_race2[melt$Par_race == 5] = "White"

melt$Subject = as.factor(melt$Subject)
melt$Task = as.factor(melt$Task)
melt$Observer = as.factor(melt$Observer)
melt$Exp_ID = as.factor(melt$Exp_ID)
melt$Exp_race2 = as.factor(melt$Exp_race2)
melt$Exp_gender = as.factor(melt$Exp_gender)
melt$Par_race2 = as.factor(melt$Par_race2)
melt$Par_race = as.factor(melt$Par_race)

explm = lmer(perfBias ~ Observer*Exp_race2*Task+(1|Subject), data = melt) %>% 
  summary()
kable(explm$coefficients, digits = 3)

```

### 9. Effect of participant race
When coded as binary (White/nonWhite), there is a significant difference in performance bias between White and nonWhite participants, moreso in the APT than the WIT, although the interaction isn't significant.  

When not coded as binary, this seems to be driven by one group in particular, but need to do a better job of coding variables to figure out why. 

``` {r}
lm(perfBias ~ Par_race2*Task, data = melt) %>% 
  summary()

ggplot(filter(melt, Task == "WIT"), aes(Subject, perfBias, color = Par_race2)) +
  geom_point() +
  geom_hline(yintercept=0)

```