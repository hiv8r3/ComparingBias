---
title: Observer analyses (Fewer subjects excluded)
author: Hannah, 2/19/2019
output:
  html_document:
    highlight: pygments
    theme: cerulean
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
options(width=140)
library(dplyr)
library(tidyr)
library(ggplot2)
library(lme4)
library(lmerTest)
library(knitr)
library(reshape2)
library(MuMIn)
library(simr)
```

### Original subject exclusions

Data for Study 1 was collected in Fall 2015. Data for Study 2 was collected in Fall 2016.

**In Study 1:**  
- 101 subjects participated  
- 2 subjects' AP data was discarded (reasons: no data because of comp malfunction, fell asleep)  
- 3 subjects' WIT data was discarded (reasons: no data because of comp malfunction, mixed up buttons, used one response button > 85% of the time)    
*This left 99 subjects with AP data and 98 subjects with WIT data.*

**In Study 2:**  
- 206 subjects participated  
- 5 subjects' AP data was discarded (reasons: no data because of comp malfunction [2 subjects], used one response button > 85% of the time [3 subjects])  
- 2 subjects' WIT data was discarded (reasons: no data because of comp malfunction, used one response button > 85% of the time)  
*This left 201 subjects with AP data and 204 subjects with WIT data.*

There were 48 trials for each condition in each task (192 trials total for each task). In Study 2, each task was split into two sections so that participants could answer anxiety questions in the middle and end of each task.

### 1. Accuracy in each task:  
#### 2 (Prime: Black/White) x 2 (Target: gun/tool or positive/negative) rANOVA  

#### Study 1:  
Only showing interaction indicating racial bias (Prime x Target). For calculation of effect sizes, see "7 Effect sizes.R"  

**WIT**  
```{r data}
# For analyses with accuracy as DV, need trials to be grouped into conditions (ie number of errors in each condition)
s1.acc = read.delim("Study1_accuracy.txt")

# separate by task
s1.acc.APT = s1.acc[s1.acc$Task == "APT",]
s1.acc.WIT = s1.acc[s1.acc$Task == "WIT",]

# remove bad subs
s1.bs.APT = c(53, 64)
s1.acc.APT = s1.acc.APT[!(s1.acc.APT$Subject %in% s1.bs.APT),]

s1.bs.WIT = c(4, 28, 53)
s1.acc.WIT = s1.acc.WIT[!(s1.acc.WIT$Subject %in% s1.bs.WIT),]

s1.acc.APT$Subject = factor(s1.acc.APT$Subject)
s1.acc.WIT$Subject = factor(s1.acc.WIT$Subject)

# Race x Valence on accuracy (WIT)
sum = aov(Accuracy ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s1.acc.WIT) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`

```

``` {r, eval = F}
#Calculating effect sizes
# partial-eta squared
# SS-between/(SS-between + SS-residual)

# Prime x Target interaction
1.21/(1.21+1.286)

# Effect of target following Black primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s1.acc.WIT, PrimeType == "black")) %>% 
  summary()
1.34/(1.34+1.27)

# Effect of target following White primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s1.acc.WIT, PrimeType == "white")) %>% 
  summary()
.16/(.16+1.79)
```

**AP**
``` {r AP}
# Race x Valence on accuracy (WIT)
sum = aov(Accuracy ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s1.acc.APT) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`
```
``` {r, eval = F}
#Calculating effect sizes
# partial-eta squared
# SS-between/(SS-between + SS-residual)

# Prime x Target interaction
.77/(.77+1.86)

# Effect of target following Black primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s1.acc.APT, PrimeType == "black")) %>% 
  summary()
.09/(.09+1.56)

# Effect of target following White primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s1.acc.APT, PrimeType == "white")) %>% 
  summary()
.88/(.88+2.098)

```

#### Study 2: 
Only showing interaction indicating racial bias (Prime x Target). For calculation of effect sizes, see "7 Effect sizes.R"  
**WIT**  
```{r data2, echo = FALSE}
s2.acc = read.delim("Study2_accuracy.txt")

# separate by task
s2.acc.APT = s2.acc[s2.acc$Task == "APT",]
s2.acc.WIT = s2.acc[s2.acc$Task == "WIT",]

# remove bad subs
s2.bs.APT = c(1, 9, 37, 87, 102)
s2.acc.APT = s2.acc.APT[!(s2.acc.APT$Subject %in% s2.bs.APT),]

s2.bs.WIT = c(1, 9)
s2.acc.WIT = s2.acc.WIT[!(s2.acc.WIT$Subject %in% s2.bs.WIT),]

s2.acc.APT$Subject = factor(s2.acc.APT$Subject)
s2.acc.WIT$Subject = factor(s2.acc.WIT$Subject)

# Race x Valence on accuracy (WIT)
sum = aov(Accuracy ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s2.acc.WIT) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`
```

``` {r, eval = F}
#Calculating effect sizes
# partial-eta squared
# SS-between/(SS-between + SS-residual)

# Prime x Target interaction
1.635/(1.635+2.8)

# Effect of target following Black primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s2.acc.WIT, PrimeType == "black")) %>% 
  summary()
1.956/(1.956+3.323)

# Effect of target following White primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s2.acc.WIT, PrimeType == "white")) %>% 
  summary()
.168/(.168+3.88)
```

**AP**
``` {r AP2}
# Race x Valence on accuracy (WIT)
sum = aov(Accuracy ~ PrimeType*TargetType + Error(Subject/(PrimeType*TargetType)), data = s2.acc.APT) %>% 
  summary()

sum$`Error: Subject:PrimeType:TargetType`
```

``` {r, eval = F}
#Calculating effect sizes
# partial-eta squared
# SS-between/(SS-between + SS-residual)

# Prime x Target interaction
1.05/(1.05+3.899)

# Effect of target following Black primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s2.acc.APT, PrimeType == "black")) %>% 
  summary()


# Effect of target following White primes
aov(Accuracy ~ TargetType + Error(Subject/(TargetType)), data = filter(s2.acc.APT, PrimeType == "white")) %>% 
  summary()
2.12/(2.12+4.15)
```
### 2. Comparing accuracy across tasks

#### Study 1
Excludes subjects that don't have data in both tasks (only includes sample of 97).  

#### A. Correlation between performance bias scores on each task
``` {r perfBias.s1}
s1.perfBias = read.delim("Study1_perfBias.txt")

# take out bs for both APT and WIT
s1.perfBias = s1.perfBias[!(s1.perfBias$Subject %in% s1.bs.APT),]
s1.perfBias = s1.perfBias[!(s1.perfBias$Subject %in% s1.bs.WIT),]

# standardize perfBias estimates (depends on who is included in the sample, so needs to be done after subs are excluded)
s1.perfBias$WITStand = scale(s1.perfBias$WITperfBias)
s1.perfBias$APStand = scale(s1.perfBias$APperfBias)

# Look at correlation between tasks
ggplot(s1.perfBias, aes(APStand, WITStand)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "Stand. Performance bias on AP", y = "Stand. Performance bias on WIT") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(APStand ~ WITStand, data = s1.perfBias) %>%
  summary()
```

#### B. Examine 3 way Prime x Target x Task interaction  
``` {r 3way.s1}
# Look just at subjects that have data for both tasks, otherwise throws error ("Error() model is singular")
# Probably because some subjects don't have data across both levels of task
s1.acc.nobs = s1.acc[!(s1.acc$Subject %in% s1.bs.WIT) & !(s1.acc$Subject %in% s1.bs.APT),]
s1.acc.nobs$Subject = factor(s1.acc.nobs$Subject)

# Total number of errors (looking at three-way interaction)
facet_labels <- c(APT = "APT", WIT = "WIT")

# Figure 1 ----------------------------------------------------------------
ggplot(s1.acc.nobs, aes(PrimeType, Accuracy, fill = ConType)) +
  stat_summary(fun.y = mean, geom = "bar", position = "dodge") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", position = position_dodge(width=.9), width = .2) +
  facet_wrap(~Task, labeller=labeller(Task = facet_labels)) + 
  #  ggtitle("Total number of errors") +
  labs(y = "Accuracy rate", x = "Race of Prime") +
  scale_fill_manual(values=c("black","grey70"), guide = guide_legend(title = NULL)) +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        strip.text.x = element_text(face = "bold", size = 14),
        strip.background = element_rect(fill = "grey98"),
        axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))

ggsave("./Figures/Fewer Excluded Subs/Accuracy_Study1.tiff")


# See if pattern of racial bias differs across two tasks- TOTAL ERRORS
sum2 = aov(Accuracy ~ (PrimeType*ConType*Task)+Error(Subject/(PrimeType*ConType*Task)), data = s1.acc.nobs) %>%
  summary()

sum2$`Error: Subject:PrimeType:ConType:Task`
```

``` {r eval=F}
# partial eta squared
.488/(.488+1.744)

# check correlation between conditions for sensitivity power analysis
wide = spread(select(s1.acc.nobs, Subject, TrialType, Accuracy), TrialType, Accuracy)

rs = cor(select(wide, -Subject))
```

#### Study 2
Excludes subjects that don't have data in both tasks (only includes sample of 201).  

#### A. Correlation between performance bias scores on each task
``` {r perfBias.s2}
s2.perfBias = read.delim("Study2_perfBias.txt")

# take out bs for both APT and WIT
s2.perfBias = s2.perfBias[!(s2.perfBias$Subject %in% s2.bs.APT),]
s2.perfBias = s2.perfBias[!(s2.perfBias$Subject %in% s2.bs.WIT),]

# standardize perfBias estimates (depends on who is included in the sample, so needs to be done after subs are excluded)
s2.perfBias$WITStand = scale(s2.perfBias$WITperfBias)
s2.perfBias$APStand = scale(s2.perfBias$APperfBias)

# Look at correlation between tasks
ggplot(s2.perfBias, aes(APStand, WITStand)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "Stand. Performance bias on AP", y = "Stand. Performance bias on WIT") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(APStand ~ WITStand, data = s2.perfBias) %>%
  summary()
```

#### B. Examine 3 way Prime x Target x Task interaction  
``` {r 3way.s2}
# Look just at subjects that have data for both tasks, otherwise throws error ("Error() model is singular")
# Probably because some subjects don't have data across both levels of task
s2.acc.nobs = s2.acc[!(s2.acc$Subject %in% s2.bs.WIT) & !(s2.acc$Subject %in% s2.bs.APT),]
s2.acc.nobs$Subject = factor(s2.acc.nobs$Subject)

# Total number of errors (looking at three-way interaction)
facet_labels <- c(APT = "APT", WIT = "WIT")

# Figure 1 ----------------------------------------------------------------
ggplot(s2.acc.nobs, aes(PrimeType, Accuracy, fill = ConType)) +
  stat_summary(fun.y = mean, geom = "bar", position = "dodge") +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", position = position_dodge(width=.9), width = .2) +
  facet_wrap(~Task, labeller=labeller(Task = facet_labels)) + 
  #  ggtitle("Total number of errors") +
  labs(y = "Accuracy rate", x = "Race of Prime") +
  scale_fill_manual(values=c("black","grey70"), guide = guide_legend(title = NULL)) +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        strip.text.x = element_text(face = "bold", size = 14),
        strip.background = element_rect(fill = "grey98"),
        axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14))

ggsave("./Figures/Fewer Excluded Subs/Accuracy_Study2.tiff")


# See if pattern of racial bias differs across two tasks- TOTAL ERRORS
sum2 = aov(Accuracy ~ (PrimeType*ConType*Task)+Error(Subject/(PrimeType*ConType*Task)), data = s2.acc.nobs) %>%
  summary()

sum2$`Error: Subject:PrimeType:ConType:Task`

```

```{r, eval = F}
# partial eta squared
1.458/(1.458+2.996)
```
### 3. Correspondance in PDP estimates across tasks  
Only includes data for participants with data in both tasks (N = 97).  

####Study 1:  
``` {r correl}

## CHANGES TO PDP ESTIMATE CALCULATION SINCE INTIIAL SUBMISSION:
# 1. TOOK OUT MISS TRIALS (IE ONLY INCLUDED TRIALS IN WHICH A RESPONSE WAS RECORDED WITHIN THE TIME WINDOW)
# 2. REPLACED NEGATIVE ESTIMATES WITH TRIALS

# compare PDP-C estimates across tasks
s1.PDP.APT = read.delim("Study1_pdpEstimates_APT.txt")
s1.PDP.WIT = read.delim("Study1_pdpEstimates_WIT.txt")

# take out subs
s1.PDP.APT = s1.PDP.APT[!(s1.PDP.APT$Subject %in% s1.bs.WIT) & !(s1.PDP.APT$Subject %in% s1.bs.APT),]

s1.PDP.WIT = s1.PDP.WIT[!(s1.PDP.WIT$Subject %in% s1.bs.WIT) & !(s1.PDP.WIT$Subject %in% s1.bs.APT),]

# Add standarized estimates (depends on subjects included)
s1.PDP.APT$Cmean.scale = scale(s1.PDP.APT$C_mean)
s1.PDP.WIT$Cmean.scale = scale(s1.PDP.WIT$C_mean)
s1.PDP.APT$AResid.scale = scale(s1.PDP.APT$AResid)
s1.PDP.WIT$AResid.scale = scale(s1.PDP.WIT$AResid)

# C estimates
s1.Cest = 
  cbind(rename(s1.PDP.APT, APT.Cmean.scale = Cmean.scale) %>% 
        select(APT.Cmean.scale),
      rename(s1.PDP.WIT, WIT.Cmean.scale = Cmean.scale) %>% 
        select(WIT.Cmean.scale, Subject))

ggplot(s1.Cest, aes(WIT.Cmean.scale, APT.Cmean.scale)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-C", y = "APT PDP-C") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT.Cmean.scale ~ APT.Cmean.scale, data = s1.Cest) %>% summary()

# A estimates
s1.Aest = 
  cbind(rename(s1.PDP.APT, APT.AResid.scale = AResid.scale) %>% 
        select(APT.AResid.scale),
      rename(s1.PDP.WIT, WIT.AResid.scale = AResid.scale) %>% 
        select(WIT.AResid.scale, Subject))

ggplot(s1.Aest, aes(WIT.AResid.scale, APT.AResid.scale)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-A", y = "APT PDP-A") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT.AResid.scale ~ APT.AResid.scale, data = s1.Aest) %>% summary()


# Compare C and A resid 
s1.Aest$EstType = "A"
names(s1.Aest) = c("APT", "WIT", "Subject", "EstType")

s1.Cest$EstType = "C"
names(s1.Cest) = c("APT", "WIT", "Subject", "EstType")

compareAC = rbind(s1.Aest, s1.Cest)

ggplot(compareAC, aes(WIT, APT, pch = EstType)) +
  geom_point(aes(shape = EstType), size = 2.5, alpha = .7) +
  scale_shape_manual(values=c(1,17)) +
  scale_linetype_manual(values=c("solid", "dashed")) +
  theme_bw() +
  geom_smooth(method = "lm", aes(linetype=EstType), color = "black", fullrange = T) +
  labs(x = "PDP estimates for WIT", y = "PDP estimates for APT") +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        legend.title = element_blank(),
        legend.key.size = unit(1.2, "cm"),
        axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        axis.text.x = element_text(size=16),
        axis.text.y = element_text(size=16))+
  coord_cartesian(expand=c(0,0))

ggsave("./Figures/Fewer Excluded Subs/Study1_CompareAC.tiff")

sum = lmer(WIT ~ APT*EstType+(1|Subject), data = compareAC) %>% summary()
sum$call
sum$coefficients

```
``` {r eval=F}
# run other models to get f^2 (measures local effect size)
# f^2 = (R^2(full) - R^2(reduced))/(1-R^2(full))
# where full has all predictors/interactions and reduced has all but the one you want the effect size for

full = lmer(WIT ~ APT*EstType+(1|Subject), data = compareAC) %>% r.squaredGLMM()
reduced = lmer(WIT ~ APT+EstType+(1|Subject), data = compareAC) %>% r.squaredGLMM() 

(full[2] - reduced[2])/(1-full[2])


# power simulation
model1 = lmer(WIT ~ APT*EstType+(1|Subject), data = compareAC)
powerSim(model1, test=fixed("APT:EstTypeC", method = "z"), nsim=100)

```
####Study 2:  
``` {r correl2}
# compare PDP-C estimates across tasks
s2.PDP.APT = read.delim("Study2_pdpEstimates_APT.txt")
s2.PDP.WIT = read.delim("Study2_pdpEstimates_WIT.txt")

# take out subs with missing data
s2.PDP.APT = s2.PDP.APT[!(s2.PDP.APT$Subject %in% s2.bs.WIT) & !(s2.PDP.APT$Subject %in% s2.bs.APT),]

s2.PDP.WIT = s2.PDP.WIT[!(s2.PDP.WIT$Subject %in% s2.bs.WIT) & !(s2.PDP.WIT$Subject %in% s2.bs.APT),]

# Add standarized estimates (depends on subjects included)
s2.PDP.APT$Cmean.scale = scale(s2.PDP.APT$C_mean)
s2.PDP.WIT$Cmean.scale = scale(s2.PDP.WIT$C_mean)
s2.PDP.APT$AResid.scale = scale(s2.PDP.APT$AResid)
s2.PDP.WIT$AResid.scale = scale(s2.PDP.WIT$AResid)

# C estimates
s2.Cest = 
  cbind(rename(s2.PDP.APT, APT.Cmean.scale = Cmean.scale) %>% 
        select(APT.Cmean.scale),
      rename(s2.PDP.WIT, WIT.Cmean.scale = Cmean.scale) %>% 
        select(WIT.Cmean.scale, Subject))

ggplot(s2.Cest, aes(WIT.Cmean.scale, APT.Cmean.scale)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-C", y = "APT PDP-C") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT.Cmean.scale ~ APT.Cmean.scale, data = s2.Cest) %>% summary()

# A estimates
s2.Aest = 
  cbind(rename(s2.PDP.APT, APT.AResid.scale = AResid.scale) %>% 
        select(APT.AResid.scale),
      rename(s2.PDP.WIT, WIT.AResid.scale = AResid.scale) %>% 
        select(WIT.AResid.scale, Subject))

ggplot(s2.Aest, aes(WIT.AResid.scale, APT.AResid.scale)) +
  geom_point() +
  geom_smooth(method = "lm") +
  #  ggtitle("Correlation between accuracy on WIT and accuracy on AP") +
  labs(x = "WIT PDP-A", y = "APT PDP-A") +
  theme_bw()+
  theme(axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        title = element_text(size=20)
        #axis.text.x  = element_text(angle=90, vjust=0.5, size=16)
  )

lm(WIT.AResid.scale ~ APT.AResid.scale, data = s2.Aest) %>% summary()


# Compare C and A resid 
s2.Aest$EstType = "A"
names(s2.Aest) = c("APT", "WIT", "Subject", "EstType")

s2.Cest$EstType = "C"
names(s2.Cest) = c("APT", "WIT", "Subject", "EstType")

compareAC = rbind(s2.Aest, s2.Cest)

ggplot(compareAC, aes(WIT, APT, pch = EstType)) +
  geom_point(aes(shape = EstType), size = 2.5, alpha = .7) +
  scale_shape_manual(values=c(1,17)) +
  scale_linetype_manual(values=c("solid", "dashed")) +
  theme_bw() +
  geom_smooth(method = "lm", aes(linetype=EstType), color = "black", fullrange = T) +
  labs(x = "PDP estimates for WIT", y = "PDP estimates for APT") +
  theme(panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_line(color = "white"),
        legend.title = element_blank(),
        legend.key.size = unit(1.2, "cm"),
        axis.title.x = element_text(size=20),
        axis.title.y = element_text(size=20),
        axis.text.x = element_text(size=16),
        axis.text.y = element_text(size=16))+
  coord_cartesian(expand=c(0,0))

ggsave("./Figures/Fewer Excluded Subs/Study2_CompareAC.tiff")

sum = lmer(WIT ~ APT*EstType+(1|Subject), data = compareAC) %>% summary()
sum$call
sum$coefficients

```
``` {r eval=F}
# run other models to get f^2 (measures local effect size)
# f^2 = (R^2(full) - R^2(reduced))/(1-R^2(full))
# where full has all predictors/interactions and reduced has all but the one you want the effect size for

full = lmer(WIT ~ APT*EstType+(1|Subject), data = compareAC) %>% r.squaredGLMM()
reduced = lmer(WIT ~ APT+EstType+(1|Subject), data = compareAC) %>% r.squaredGLMM() 

(full[2] - reduced[2])/(1-full[2])

# power simulation
model1 = lmer(WIT ~ APT*EstType+(1|Subject), data = compareAC)
powerSim(model1, test=fixed("APT:EstTypeC", method = "z"), nsim=100)

```

